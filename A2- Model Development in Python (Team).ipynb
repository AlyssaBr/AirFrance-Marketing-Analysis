{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcfe32ef",
   "metadata": {},
   "source": [
    "# **MODEL INTRODUCTION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91624ae8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In this analysis, our objective was to predict the \"Total Volume of Bookings\" for an airline based on various features related to online advertising campaigns. We considered multiple regression models, including Lasso, Ridge, ElasticNet, and Lars, among others, to identify the most effective model for our dataset. Our final choice, the Least Angle Regression (LARS) model, was selected for its efficiency in dealing with high-dimensional data and its ability to perform feature selection, which is crucial in datasets with many predictors. The LARS model is particularly suited for scenarios where the number of observations is significantly less than the number of features, or when multicollinearity is present among the feature variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8c5edc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing libraries ##\n",
    "\n",
    "# for this template\n",
    "import numpy             as np                       # mathematical essentials\n",
    "import pandas            as pd                       # data science essentials\n",
    "import sklearn.linear_model                          # linear models\n",
    "from sklearn.model_selection import train_test_split # train/test split\n",
    "\n",
    "\n",
    "#!###############################!#\n",
    "#!# import additional libraries #!#\n",
    "#!###############################!#\n",
    "# import whatever you need\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "from sklearn.preprocessing import StandardScaler  # standard scaler\n",
    "from sklearn.model_selection import train_test_split   # train-test split\n",
    "import warnings\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV # hyperparameter tuning\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, SGDRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor         # regression trees\n",
    "from sklearn.tree import plot_tree                     # tree plots\n",
    "from scipy.stats import uniform\n",
    "from sklearn import linear_model, neighbors, tree\n",
    "from sklearn.linear_model import BayesianRidge, TheilSenRegressor, ARDRegression, PassiveAggressiveRegressor, RANSACRegressor, OrthogonalMatchingPursuit, LassoLars, HuberRegressor, ElasticNet\n",
    "\n",
    "\n",
    "#from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error, r2_score, make_scorer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "# setting pandas print options (optional)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37689a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LinearRegression': 2.327449413665459,\n",
       " 'Ridge': 1.2647411212922004,\n",
       " 'Lasso': 1.128496823760913,\n",
       " 'KNeighborsRegressor': 1.9049064426477207,\n",
       " 'DecisionTreeRegressor': 1.038221403516392}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Reload the data\n",
    "train_df = pd.read_csv('/Users/nikishah/Desktop/training_data.csv')\n",
    "test_df = pd.read_csv('/Users/nikishah/Desktop/testing_data.csv')\n",
    "\n",
    "# Define feature columns (excluding 'entry_id' and the target variable 'Total Volume of Bookings')\n",
    "feature_cols = train_df.columns.drop(['entry_id', 'Total Volume of Bookings'])\n",
    "\n",
    "# Define categorical and numerical columns\n",
    "categorical_cols = train_df[feature_cols].select_dtypes(include=['object']).columns\n",
    "numerical_cols = train_df[feature_cols].select_dtypes(exclude=['object']).columns\n",
    "\n",
    "# Preprocessing for numerical data\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "])\n",
    "\n",
    "# Split the data into features and target variable\n",
    "X = train_df[feature_cols]\n",
    "y = train_df['Total Volume of Bookings']\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Define models to evaluate\n",
    "models = {\n",
    "    'LinearRegression': LinearRegression(),\n",
    "    'Ridge': Ridge(),\n",
    "    'Lasso': Lasso(),\n",
    "    'KNeighborsRegressor': KNeighborsRegressor(),\n",
    "    'DecisionTreeRegressor': DecisionTreeRegressor()\n",
    "}\n",
    "\n",
    "# Evaluate models\n",
    "model_performance = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    # Create and fit the pipeline\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('model', model)])\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions and evaluate the model\n",
    "    preds = pipeline.predict(X_valid)\n",
    "    mse = mean_squared_error(y_valid, preds)\n",
    "    rmse = mse ** 0.5\n",
    "    model_performance[model_name] = rmse\n",
    "\n",
    "model_performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46381862",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entry_id</th>\n",
       "      <th>Publisher Name</th>\n",
       "      <th>Keyword</th>\n",
       "      <th>Match Type</th>\n",
       "      <th>Campaign</th>\n",
       "      <th>Keyword Group</th>\n",
       "      <th>Category</th>\n",
       "      <th>Bid Strategy</th>\n",
       "      <th>Status</th>\n",
       "      <th>Search Engine Bid</th>\n",
       "      <th>Clicks</th>\n",
       "      <th>Click Charges</th>\n",
       "      <th>Avg. Cost per Click</th>\n",
       "      <th>Impressions</th>\n",
       "      <th>Engine Click Thru %</th>\n",
       "      <th>Avg. Pos.</th>\n",
       "      <th>Trans. Conv. %</th>\n",
       "      <th>Total Cost/ Trans.</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Total Cost</th>\n",
       "      <th>Total Volume of Bookings</th>\n",
       "      <th>Click Charge Ratio</th>\n",
       "      <th>Log Impressions</th>\n",
       "      <th>Bid Strategy Factor</th>\n",
       "      <th>Interaction</th>\n",
       "      <th>Search Engine Bid Cut</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mkt_007</td>\n",
       "      <td>Google - US</td>\n",
       "      <td>air france</td>\n",
       "      <td>Broad</td>\n",
       "      <td>Air France Branded</td>\n",
       "      <td>Air France Brand</td>\n",
       "      <td>uncategorized</td>\n",
       "      <td>Position 5-10 Bid Strategy</td>\n",
       "      <td>Live</td>\n",
       "      <td>27.5000</td>\n",
       "      <td>29060</td>\n",
       "      <td>46188.437315</td>\n",
       "      <td>1.589416</td>\n",
       "      <td>385476</td>\n",
       "      <td>7.538731</td>\n",
       "      <td>1.438942</td>\n",
       "      <td>0.770819</td>\n",
       "      <td>206.198381</td>\n",
       "      <td>290609.9</td>\n",
       "      <td>46188.437315</td>\n",
       "      <td>224</td>\n",
       "      <td>1.589416</td>\n",
       "      <td>12.862234</td>\n",
       "      <td>Position 5-10 Bid Strategy</td>\n",
       "      <td>52.771119</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mkt_009</td>\n",
       "      <td>Overture - US</td>\n",
       "      <td>airplane tiket</td>\n",
       "      <td>Standard</td>\n",
       "      <td>Unassigned</td>\n",
       "      <td>Unassigned</td>\n",
       "      <td>airgeneral</td>\n",
       "      <td>Position 5-10 Bid Strategy</td>\n",
       "      <td>Paused</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>2</td>\n",
       "      <td>1.125000</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>59</td>\n",
       "      <td>3.389831</td>\n",
       "      <td>1.754237</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.125000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>4.077537</td>\n",
       "      <td>Position 5-10 Bid Strategy</td>\n",
       "      <td>23.728814</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mkt_015</td>\n",
       "      <td>Google - US</td>\n",
       "      <td>rome plane tickets</td>\n",
       "      <td>Broad</td>\n",
       "      <td>Google_Yearlong 2006</td>\n",
       "      <td>Google|Rome</td>\n",
       "      <td>uncategorized</td>\n",
       "      <td>Position 5-10 Bid Strategy</td>\n",
       "      <td>Paused</td>\n",
       "      <td>6.2500</td>\n",
       "      <td>1</td>\n",
       "      <td>1.687500</td>\n",
       "      <td>1.687500</td>\n",
       "      <td>14</td>\n",
       "      <td>7.142857</td>\n",
       "      <td>1.714286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.687500</td>\n",
       "      <td>0</td>\n",
       "      <td>1.687500</td>\n",
       "      <td>2.639057</td>\n",
       "      <td>Position 5-10 Bid Strategy</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mkt_017</td>\n",
       "      <td>Google - US</td>\n",
       "      <td>barcelona airlines</td>\n",
       "      <td>Broad</td>\n",
       "      <td>Google_Yearlong 2006</td>\n",
       "      <td>Google|Barcelona</td>\n",
       "      <td>uncategorized</td>\n",
       "      <td>Position 5-10 Bid Strategy</td>\n",
       "      <td>Paused</td>\n",
       "      <td>6.2500</td>\n",
       "      <td>93</td>\n",
       "      <td>253.212500</td>\n",
       "      <td>2.722715</td>\n",
       "      <td>2704</td>\n",
       "      <td>3.439349</td>\n",
       "      <td>2.130030</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>253.212500</td>\n",
       "      <td>0</td>\n",
       "      <td>2.722715</td>\n",
       "      <td>7.902487</td>\n",
       "      <td>Position 5-10 Bid Strategy</td>\n",
       "      <td>24.075444</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mkt_023</td>\n",
       "      <td>Overture - US</td>\n",
       "      <td>discount england flight</td>\n",
       "      <td>Advanced</td>\n",
       "      <td>Unassigned</td>\n",
       "      <td>Unassigned</td>\n",
       "      <td>discount</td>\n",
       "      <td>Position 5-10 Bid Strategy</td>\n",
       "      <td>Paused</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>4</td>\n",
       "      <td>2.937500</td>\n",
       "      <td>0.734375</td>\n",
       "      <td>169</td>\n",
       "      <td>2.366864</td>\n",
       "      <td>3.084615</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.937500</td>\n",
       "      <td>0</td>\n",
       "      <td>0.734375</td>\n",
       "      <td>5.129899</td>\n",
       "      <td>Position 5-10 Bid Strategy</td>\n",
       "      <td>16.568047</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  entry_id Publisher Name                  Keyword Match Type              Campaign     Keyword Group       Category                Bid Strategy  Status  Search Engine Bid  Clicks  Click Charges  Avg. Cost per Click  Impressions  Engine Click Thru %  Avg. Pos.  Trans. Conv. %  Total Cost/ Trans.    Amount    Total Cost  Total Volume of Bookings  Click Charge Ratio  Log Impressions         Bid Strategy Factor  Interaction  Search Engine Bid Cut\n",
       "0  mkt_007    Google - US               air france      Broad    Air France Branded  Air France Brand  uncategorized  Position 5-10 Bid Strategy    Live            27.5000   29060   46188.437315             1.589416       385476             7.538731   1.438942        0.770819          206.198381  290609.9  46188.437315                       224            1.589416        12.862234  Position 5-10 Bid Strategy    52.771119                      4\n",
       "1  mkt_009  Overture - US           airplane tiket   Standard            Unassigned        Unassigned     airgeneral  Position 5-10 Bid Strategy  Paused             0.1250       2       1.125000             0.562500           59             3.389831   1.754237        0.000000            0.000000       0.0      1.125000                         0            0.562500         4.077537  Position 5-10 Bid Strategy    23.728814                      1\n",
       "2  mkt_015    Google - US       rome plane tickets      Broad  Google_Yearlong 2006       Google|Rome  uncategorized  Position 5-10 Bid Strategy  Paused             6.2500       1       1.687500             1.687500           14             7.142857   1.714286        0.000000            0.000000       0.0      1.687500                         0            1.687500         2.639057  Position 5-10 Bid Strategy    50.000000                      2\n",
       "3  mkt_017    Google - US       barcelona airlines      Broad  Google_Yearlong 2006  Google|Barcelona  uncategorized  Position 5-10 Bid Strategy  Paused             6.2500      93     253.212500             2.722715         2704             3.439349   2.130030        0.000000            0.000000       0.0    253.212500                         0            2.722715         7.902487  Position 5-10 Bid Strategy    24.075444                      2\n",
       "4  mkt_023  Overture - US  discount england flight   Advanced            Unassigned        Unassigned       discount  Position 5-10 Bid Strategy  Paused             0.1875       4       2.937500             0.734375          169             2.366864   3.084615        0.000000            0.000000       0.0      2.937500                         0            0.734375         5.129899  Position 5-10 Bid Strategy    16.568047                      1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/nikishah/Desktop/training_data.csv',  encoding=\"cp1252\")\n",
    "df = pd.read_csv('/Users/nikishah/Desktop/testing_data.csv',  encoding=\"cp1252\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "261c4e96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Search Engine Bid</th>\n",
       "      <th>Clicks</th>\n",
       "      <th>Click Charges</th>\n",
       "      <th>Avg. Cost per Click</th>\n",
       "      <th>Impressions</th>\n",
       "      <th>Engine Click Thru %</th>\n",
       "      <th>Avg. Pos.</th>\n",
       "      <th>Trans. Conv. %</th>\n",
       "      <th>Total Cost/ Trans.</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Total Cost</th>\n",
       "      <th>Total Volume of Bookings</th>\n",
       "      <th>Click Charge Ratio</th>\n",
       "      <th>Log Impressions</th>\n",
       "      <th>Interaction</th>\n",
       "      <th>Search Engine Bid Cut</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>882.000000</td>\n",
       "      <td>882.000000</td>\n",
       "      <td>882.000000</td>\n",
       "      <td>882.000000</td>\n",
       "      <td>8.820000e+02</td>\n",
       "      <td>882.000000</td>\n",
       "      <td>882.000000</td>\n",
       "      <td>882.000000</td>\n",
       "      <td>882.000000</td>\n",
       "      <td>882.000000</td>\n",
       "      <td>882.000000</td>\n",
       "      <td>882.000000</td>\n",
       "      <td>882.000000</td>\n",
       "      <td>882.000000</td>\n",
       "      <td>882.000000</td>\n",
       "      <td>882.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.455527</td>\n",
       "      <td>199.081633</td>\n",
       "      <td>285.138165</td>\n",
       "      <td>1.914164</td>\n",
       "      <td>9.069955e+03</td>\n",
       "      <td>11.548085</td>\n",
       "      <td>1.880578</td>\n",
       "      <td>0.295099</td>\n",
       "      <td>42.472772</td>\n",
       "      <td>1907.136905</td>\n",
       "      <td>285.138165</td>\n",
       "      <td>1.580499</td>\n",
       "      <td>1.914164</td>\n",
       "      <td>5.096188</td>\n",
       "      <td>77.751426</td>\n",
       "      <td>1.956916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.344341</td>\n",
       "      <td>1915.925603</td>\n",
       "      <td>2353.344330</td>\n",
       "      <td>1.262732</td>\n",
       "      <td>1.526663e+05</td>\n",
       "      <td>20.497278</td>\n",
       "      <td>1.037800</td>\n",
       "      <td>2.688710</td>\n",
       "      <td>400.255362</td>\n",
       "      <td>22862.943408</td>\n",
       "      <td>2353.344330</td>\n",
       "      <td>19.083507</td>\n",
       "      <td>1.262732</td>\n",
       "      <td>2.600165</td>\n",
       "      <td>141.951847</td>\n",
       "      <td>0.680824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.037216</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.195432</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.378125</td>\n",
       "      <td>0.881875</td>\n",
       "      <td>2.400000e+01</td>\n",
       "      <td>1.654983</td>\n",
       "      <td>1.128149</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.378125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.881875</td>\n",
       "      <td>3.178054</td>\n",
       "      <td>9.673241</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.250000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>1.687500</td>\n",
       "      <td>1.715000e+02</td>\n",
       "      <td>4.342069</td>\n",
       "      <td>1.582071</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.687500</td>\n",
       "      <td>5.144545</td>\n",
       "      <td>26.603476</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.250000</td>\n",
       "      <td>20.750000</td>\n",
       "      <td>36.396875</td>\n",
       "      <td>2.720161</td>\n",
       "      <td>8.262500e+02</td>\n",
       "      <td>11.111111</td>\n",
       "      <td>2.171715</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>36.396875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.720161</td>\n",
       "      <td>6.716896</td>\n",
       "      <td>75.650480</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>27.500000</td>\n",
       "      <td>34012.000000</td>\n",
       "      <td>46188.437315</td>\n",
       "      <td>6.358333</td>\n",
       "      <td>4.492536e+06</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>7.216074</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>9597.174987</td>\n",
       "      <td>515791.900000</td>\n",
       "      <td>46188.437315</td>\n",
       "      <td>439.000000</td>\n",
       "      <td>6.358333</td>\n",
       "      <td>15.317928</td>\n",
       "      <td>1400.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Search Engine Bid        Clicks  Click Charges  Avg. Cost per Click   Impressions  Engine Click Thru %   Avg. Pos.  Trans. Conv. %  Total Cost/ Trans.         Amount    Total Cost  Total Volume of Bookings  Click Charge Ratio  Log Impressions  Interaction  Search Engine Bid Cut\n",
       "count         882.000000    882.000000     882.000000           882.000000  8.820000e+02           882.000000  882.000000      882.000000          882.000000     882.000000    882.000000                882.000000          882.000000       882.000000   882.000000             882.000000\n",
       "mean            5.455527    199.081633     285.138165             1.914164  9.069955e+03            11.548085    1.880578        0.295099           42.472772    1907.136905    285.138165                  1.580499            1.914164         5.096188    77.751426               1.956916\n",
       "std             3.344341   1915.925603    2353.344330             1.262732  1.526663e+05            20.497278    1.037800        2.688710          400.255362   22862.943408   2353.344330                 19.083507            1.262732         2.600165   141.951847               0.680824\n",
       "min             0.000000      1.000000       0.050000             0.050000  1.000000e+00             0.037216    0.666667        0.000000            0.000000       0.000000      0.050000                  0.000000            0.050000         0.000000     0.195432               1.000000\n",
       "25%             5.000000      1.000000       2.378125             0.881875  2.400000e+01             1.654983    1.128149        0.000000            0.000000       0.000000      2.378125                  0.000000            0.881875         3.178054     9.673241               2.000000\n",
       "50%             6.250000      4.000000       7.400000             1.687500  1.715000e+02             4.342069    1.582071        0.000000            0.000000       0.000000      7.400000                  0.000000            1.687500         5.144545    26.603476               2.000000\n",
       "75%             6.250000     20.750000      36.396875             2.720161  8.262500e+02            11.111111    2.171715        0.000000            0.000000       0.000000     36.396875                  0.000000            2.720161         6.716896    75.650480               2.000000\n",
       "max            27.500000  34012.000000   46188.437315             6.358333  4.492536e+06           200.000000    7.216074       50.000000         9597.174987  515791.900000  46188.437315                439.000000            6.358333        15.317928  1400.000000               4.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cf21768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 882 entries, 0 to 881\n",
      "Data columns (total 26 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   entry_id                  882 non-null    object \n",
      " 1   Publisher Name            882 non-null    object \n",
      " 2   Keyword                   882 non-null    object \n",
      " 3   Match Type                871 non-null    object \n",
      " 4   Campaign                  882 non-null    object \n",
      " 5   Keyword Group             882 non-null    object \n",
      " 6   Category                  882 non-null    object \n",
      " 7   Bid Strategy              882 non-null    object \n",
      " 8   Status                    882 non-null    object \n",
      " 9   Search Engine Bid         882 non-null    float64\n",
      " 10  Clicks                    882 non-null    int64  \n",
      " 11  Click Charges             882 non-null    float64\n",
      " 12  Avg. Cost per Click       882 non-null    float64\n",
      " 13  Impressions               882 non-null    int64  \n",
      " 14  Engine Click Thru %       882 non-null    float64\n",
      " 15  Avg. Pos.                 882 non-null    float64\n",
      " 16  Trans. Conv. %            882 non-null    float64\n",
      " 17  Total Cost/ Trans.        882 non-null    float64\n",
      " 18  Amount                    882 non-null    float64\n",
      " 19  Total Cost                882 non-null    float64\n",
      " 20  Total Volume of Bookings  882 non-null    int64  \n",
      " 21  Click Charge Ratio        882 non-null    float64\n",
      " 22  Log Impressions           882 non-null    float64\n",
      " 23  Bid Strategy Factor       882 non-null    object \n",
      " 24  Interaction               882 non-null    float64\n",
      " 25  Search Engine Bid Cut     882 non-null    int64  \n",
      "dtypes: float64(12), int64(4), object(10)\n",
      "memory usage: 179.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e161f935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Publisher Name', 'Keyword', 'Match Type', 'Campaign', 'Keyword Group', 'Category', 'Bid Strategy', 'Status', 'Search Engine Bid', 'Clicks', 'Click Charges', 'Avg. Cost per Click', 'Impressions', 'Engine Click Thru %', 'Avg. Pos.', 'Trans. Conv. %', 'Total Cost/ Trans.', 'Amount', 'Total Cost', 'Click Charge Ratio', 'Log Impressions', 'Bid Strategy Factor', 'Interaction', 'Search Engine Bid Cut'], dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_features = df.drop(['entry_id', 'Total Volume of Bookings'], axis=1).columns\n",
    "y_variable = 'Total Volume of Bookings'\n",
    "x_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a2eeef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming your original dataset is loaded into 'df'\n",
    "# Replace 'df' with the correct variable name if different\n",
    "\n",
    "y_data = df[y_variable]\n",
    "\n",
    "# Selecting standardizing of x_data, or not. 1: standardizing 0: no-standardizing\n",
    "setting_std = 1\n",
    "\n",
    "# Standardizing the data\n",
    "if setting_std == 1:\n",
    "    scaler = StandardScaler()\n",
    "    df_std = df.copy()\n",
    "    numeric_cols = df_std.select_dtypes(include=[int, float]).columns\n",
    "    df_std[numeric_cols] = scaler.fit_transform(df_std[numeric_cols])\n",
    "\n",
    "# removing non-numeric columns and missing values\n",
    "if setting_std == 0:\n",
    "    x_data = df[x_features].copy().select_dtypes(include=[int, float]).dropna(axis=1)\n",
    "else:\n",
    "    x_data = df_std[x_features].copy().select_dtypes(include=[int, float]).dropna(axis=1)\n",
    "\n",
    "# storing remaining x_features after the step above\n",
    "x_features = list(x_data.columns)\n",
    "\n",
    "# train-test split (to validate the model)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.25, random_state=702)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bf375be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_data shape: (882,)\n",
      "x_data shape: (882, 15)\n",
      "x_train shape: (661, 15), y_train shape: (661,)\n",
      "x_test shape: (221, 15), y_test shape: (221,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Search Engine Bid</th>\n",
       "      <th>Clicks</th>\n",
       "      <th>Click Charges</th>\n",
       "      <th>Avg. Cost per Click</th>\n",
       "      <th>Impressions</th>\n",
       "      <th>Engine Click Thru %</th>\n",
       "      <th>Avg. Pos.</th>\n",
       "      <th>Trans. Conv. %</th>\n",
       "      <th>Total Cost/ Trans.</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Total Cost</th>\n",
       "      <th>Click Charge Ratio</th>\n",
       "      <th>Log Impressions</th>\n",
       "      <th>Interaction</th>\n",
       "      <th>Search Engine Bid Cut</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.595314</td>\n",
       "      <td>15.072241</td>\n",
       "      <td>19.516627</td>\n",
       "      <td>-0.257325</td>\n",
       "      <td>2.466946</td>\n",
       "      <td>-0.195715</td>\n",
       "      <td>-0.425791</td>\n",
       "      <td>0.177033</td>\n",
       "      <td>0.409285</td>\n",
       "      <td>12.634706</td>\n",
       "      <td>19.516627</td>\n",
       "      <td>-0.257325</td>\n",
       "      <td>2.988446</td>\n",
       "      <td>-0.176077</td>\n",
       "      <td>3.002599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.594799</td>\n",
       "      <td>-0.102923</td>\n",
       "      <td>-0.120753</td>\n",
       "      <td>-1.071035</td>\n",
       "      <td>-0.059057</td>\n",
       "      <td>-0.398242</td>\n",
       "      <td>-0.121808</td>\n",
       "      <td>-0.109817</td>\n",
       "      <td>-0.106174</td>\n",
       "      <td>-0.083463</td>\n",
       "      <td>-0.120753</td>\n",
       "      <td>-1.071035</td>\n",
       "      <td>-0.391986</td>\n",
       "      <td>-0.380786</td>\n",
       "      <td>-1.406323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.237692</td>\n",
       "      <td>-0.103446</td>\n",
       "      <td>-0.120514</td>\n",
       "      <td>-0.179605</td>\n",
       "      <td>-0.059352</td>\n",
       "      <td>-0.215040</td>\n",
       "      <td>-0.160326</td>\n",
       "      <td>-0.109817</td>\n",
       "      <td>-0.106174</td>\n",
       "      <td>-0.083463</td>\n",
       "      <td>-0.120514</td>\n",
       "      <td>-0.179605</td>\n",
       "      <td>-0.945527</td>\n",
       "      <td>-0.195610</td>\n",
       "      <td>0.063318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.237692</td>\n",
       "      <td>-0.055400</td>\n",
       "      <td>-0.013574</td>\n",
       "      <td>0.640682</td>\n",
       "      <td>-0.041722</td>\n",
       "      <td>-0.395825</td>\n",
       "      <td>0.240502</td>\n",
       "      <td>-0.109817</td>\n",
       "      <td>-0.106174</td>\n",
       "      <td>-0.083463</td>\n",
       "      <td>-0.013574</td>\n",
       "      <td>0.640682</td>\n",
       "      <td>1.079890</td>\n",
       "      <td>-0.378343</td>\n",
       "      <td>0.063318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.576100</td>\n",
       "      <td>-0.101879</td>\n",
       "      <td>-0.119983</td>\n",
       "      <td>-0.934845</td>\n",
       "      <td>-0.058336</td>\n",
       "      <td>-0.448178</td>\n",
       "      <td>1.160841</td>\n",
       "      <td>-0.109817</td>\n",
       "      <td>-0.106174</td>\n",
       "      <td>-0.083463</td>\n",
       "      <td>-0.119983</td>\n",
       "      <td>-0.934845</td>\n",
       "      <td>0.012972</td>\n",
       "      <td>-0.431260</td>\n",
       "      <td>-1.406323</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Search Engine Bid     Clicks  Click Charges  Avg. Cost per Click  Impressions  Engine Click Thru %  Avg. Pos.  Trans. Conv. %  Total Cost/ Trans.     Amount  Total Cost  Click Charge Ratio  Log Impressions  Interaction  Search Engine Bid Cut\n",
       "0           6.595314  15.072241      19.516627            -0.257325     2.466946            -0.195715  -0.425791        0.177033            0.409285  12.634706   19.516627           -0.257325         2.988446    -0.176077               3.002599\n",
       "1          -1.594799  -0.102923      -0.120753            -1.071035    -0.059057            -0.398242  -0.121808       -0.109817           -0.106174  -0.083463   -0.120753           -1.071035        -0.391986    -0.380786              -1.406323\n",
       "2           0.237692  -0.103446      -0.120514            -0.179605    -0.059352            -0.215040  -0.160326       -0.109817           -0.106174  -0.083463   -0.120514           -0.179605        -0.945527    -0.195610               0.063318\n",
       "3           0.237692  -0.055400      -0.013574             0.640682    -0.041722            -0.395825   0.240502       -0.109817           -0.106174  -0.083463   -0.013574            0.640682         1.079890    -0.378343               0.063318\n",
       "4          -1.576100  -0.101879      -0.119983            -0.934845    -0.058336            -0.448178   1.160841       -0.109817           -0.106174  -0.083463   -0.119983           -0.934845         0.012972    -0.431260              -1.406323"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'y_data shape: {y_data.shape}')\n",
    "print(f'x_data shape: {x_data.shape}')\n",
    "print(f'x_train shape: {x_train.shape}, y_train shape: {y_train.shape}')\n",
    "print(f'x_test shape: {x_test.shape}, y_test shape: {y_test.shape}')\n",
    "x_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b369c132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Click Charges  Clicks  Impressions  Search Engine Bid  Engine Click Thru %  \\\n",
      "0             10     100         1000                  1                  0.1   \n",
      "1             20     200         2000                  2                  0.2   \n",
      "2             30     300         3000                  3                  0.3   \n",
      "3             40     400         4000                  4                  0.4   \n",
      "4             50     500         5000                  5                  0.5   \n",
      "\n",
      "   Charge_per_Click  Log_Impressions  Bid_Effectiveness  \n",
      "0          0.099999         6.908755                0.1  \n",
      "1          0.100000         7.601402                0.4  \n",
      "2          0.100000         8.006701                0.9  \n",
      "3          0.100000         8.294300                1.6  \n",
      "4          0.100000         8.517393                2.5  \n",
      "   Charge_per_Click  Log_Impressions  Bid_Effectiveness\n",
      "0          0.099999         6.908755                0.1\n",
      "1          0.100000         7.601402                0.4\n",
      "2          0.100000         8.006701                0.9\n",
      "3          0.100000         8.294300                1.6\n",
      "4          0.100000         8.517393                2.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define or load your train_data DataFrame here\n",
    "# For example, you can define it manually:\n",
    "train_data = pd.DataFrame({\n",
    "    'Click Charges': [10, 20, 30, 40, 50],\n",
    "    'Clicks': [100, 200, 300, 400, 500],\n",
    "    'Impressions': [1000, 2000, 3000, 4000, 5000],\n",
    "    'Search Engine Bid': [1, 2, 3, 4, 5],\n",
    "    'Engine Click Thru %': [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "})\n",
    "\n",
    "# Alternatively, you can load it from a CSV file\n",
    "# train_data = pd.read_csv('/path/to/your/train_data.csv')\n",
    "\n",
    "# Feature Engineering on Train Data\n",
    "\n",
    "# Feature 1: Ratio of Click Charges to Clicks\n",
    "train_data['Charge_per_Click'] = train_data['Click Charges'] / (train_data['Clicks'] + 0.001)  # Adding a small value to avoid division by zero\n",
    "\n",
    "# Feature 2: Log Transformation of Impressions\n",
    "train_data['Log_Impressions'] = np.log1p(train_data['Impressions'])  # log1p is used to handle zero Impressions\n",
    "\n",
    "# Feature 3: Interaction between Bid and Click Through Rate\n",
    "train_data['Bid_Effectiveness'] = train_data['Search Engine Bid'] * train_data['Engine Click Thru %']\n",
    "\n",
    "# Display the first few rows of the DataFrame to verify\n",
    "print(train_data.head())\n",
    "\n",
    "# Display the selected engineered features along with existing ones\n",
    "print(train_data[['Charge_per_Click', 'Log_Impressions', 'Bid_Effectiveness']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6b0e6f",
   "metadata": {},
   "source": [
    "Feature engineering, data preprocessing, and exploratory data analysis\n",
    "We used exploratory data analysis (EDA) to look at our dataset's distributions, correlations, and possible outliers in the first stage of our investigation. Through this process, it became clear that feature engineering was required to capture more intricate relationships between variables. In particular, we added features like \"Log_Impressions,\" \"Click_Charges_Clicks_Ratio,\" and the interaction term \"Bid_Click_Through_Rate_Interaction\" in an effort to improve our model with more detailed information about the ways in which various facets of advertising campaigns affect booking volumes.\n",
    "\n",
    "We fixed missing values and standardized our features during the preprocessing stage of the data to guarantee scale uniformity, which is essential for models that are susceptible to variations in feature magnitude. Log transformations applied to specific features (such as \"Impressions\") helped to normalize distributions and stabilize variance, which improved model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cdf8feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS Linear Regression\n",
      "Lasso Regression\n",
      "Ridge Regression\n",
      "Elastic Net Regression\n",
      "K-Nearest Neighbors\n",
      "Decision Tree Regressor\n",
      "Bayesian Ridge Regression\n",
      "Theil-Sen Regression\n",
      "Stochastic Gradient Descent Regression\n",
      "Random Sample Consensus (RANSAC)\n",
      "Radius Neighbors Regressor\n",
      "Passive Aggressive Regression\n",
      "Orthogonal Matching Pursuit\n",
      "Least Angle Regression (LARS)\n",
      "LassoLars Regression\n",
      "Huber Regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikishah/anaconda3/lib/python3.10/site-packages/numpy/core/numeric.py:407: RuntimeWarning: invalid value encountered in cast\n",
      "  multiarray.copyto(res, fill_value, casting='unsafe')\n",
      "/Users/nikishah/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:639: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=5.503e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/Users/nikishah/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:639: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.617e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/Users/nikishah/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:639: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.783e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/Users/nikishah/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:639: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.728e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/Users/nikishah/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:639: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.589e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/Users/nikishah/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:639: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.043e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/Users/nikishah/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:639: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=5.091e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/Users/nikishah/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:639: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.627e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/Users/nikishah/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:639: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.362e-03, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/Users/nikishah/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:639: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=7.501e-04, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/Users/nikishah/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:639: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=5.597e-04, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/Users/nikishah/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:639: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=2.656e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic Relevance Detection (ARD)\n",
      "OLS Linear Regression: R^2 score = 0.9917, RMSE = 1.5865\n",
      "Lasso Regression: R^2 score = 0.9884, RMSE = 1.8762\n",
      "Ridge Regression: R^2 score = 0.9914, RMSE = 1.6185\n",
      "Elastic Net Regression: R^2 score = 0.8700, RMSE = 6.2873\n",
      "K-Nearest Neighbors: R^2 score = 0.2625, RMSE = 14.9741\n",
      "Decision Tree Regressor: R^2 score = 0.6673, RMSE = 10.0575\n",
      "Bayesian Ridge Regression: R^2 score = 0.9917, RMSE = 1.5864\n",
      "Theil-Sen Regression: R^2 score = 0.0126, RMSE = 17.3261\n",
      "Stochastic Gradient Descent Regression: R^2 score = 0.9885, RMSE = 1.8695\n",
      "Random Sample Consensus (RANSAC): R^2 score = -0.0110, RMSE = 17.5319\n",
      "Radius Neighbors Regressor: R^2 score = -0.0109, RMSE = 17.5307\n",
      "Passive Aggressive Regression: R^2 score = 0.9808, RMSE = 2.4191\n",
      "Orthogonal Matching Pursuit: R^2 score = 0.9961, RMSE = 1.0889\n",
      "Least Angle Regression (LARS): R^2 score = 0.9967, RMSE = 1.0080\n",
      "LassoLars Regression: R^2 score = 0.9884, RMSE = 1.8762\n",
      "Huber Regression: R^2 score = 0.9894, RMSE = 1.7953\n",
      "Automatic Relevance Detection (ARD): R^2 score = 0.9923, RMSE = 1.5286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikishah/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    \"OLS Linear Regression\": linear_model.LinearRegression(),\n",
    "    \"Lasso Regression\": linear_model.Lasso(random_state=42),\n",
    "    \"Ridge Regression\": linear_model.Ridge(random_state=42),\n",
    "    \"Elastic Net Regression\": ElasticNet(),\n",
    "    \"K-Nearest Neighbors\": neighbors.KNeighborsRegressor(),\n",
    "    \"Decision Tree Regressor\": tree.DecisionTreeRegressor(random_state=42),\n",
    "    \"Bayesian Ridge Regression\": BayesianRidge(),\n",
    "    \"Theil-Sen Regression\": TheilSenRegressor(),\n",
    "    \"Stochastic Gradient Descent Regression\": linear_model.SGDRegressor(random_state=42),\n",
    "    \"Random Sample Consensus (RANSAC)\": RANSACRegressor(random_state=42),\n",
    "    \"Radius Neighbors Regressor\": neighbors.RadiusNeighborsRegressor(),\n",
    "    \"Passive Aggressive Regression\": PassiveAggressiveRegressor(random_state=42),\n",
    "    \"Orthogonal Matching Pursuit\": OrthogonalMatchingPursuit(),\n",
    "    \"Least Angle Regression (LARS)\": linear_model.Lars(),\n",
    "    \"LassoLars Regression\": LassoLars(),\n",
    "    \"Huber Regression\": HuberRegressor(),\n",
    "    \"Automatic Relevance Detection (ARD)\": ARDRegression(),\n",
    "}\n",
    "\n",
    "# Lists and dictionaries to store instances of each model and their scores\n",
    "trained_models = []\n",
    "model_scores = {}\n",
    "\n",
    "# Function to calculate RMSE\n",
    "def rmse_score(y_true, y_pred):\n",
    "    return sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "# Training models and calculating scores\n",
    "for name, model in models.items():\n",
    "    print(name)\n",
    "    # Train the model\n",
    "    model.fit(x_train, y_train)\n",
    "    # Save the trained model in the list\n",
    "    trained_models.append((name, model))\n",
    "\n",
    "    # Predict on test data\n",
    "    y_pred = model.predict(x_test)\n",
    "\n",
    "    # Calculate R^2 and RMSE scores\n",
    "    try:\n",
    "        r2 = round(r2_score(y_test, y_pred),4)\n",
    "        rmse = round(rmse_score(y_test, y_pred),4)\n",
    "    except Exception as e:\n",
    "        print(f'{name} is something wron. y_pred has NA value because of out of calculation')\n",
    "        print(f'{name}, r2:{r2}, rmse:{rmse}')\n",
    "\n",
    "    # Save scores in the dictionary\n",
    "    model_scores[name] = {'R^2': r2, 'RMSE': rmse}\n",
    "\n",
    "# Display the scores\n",
    "for model_name, scores in model_scores.items():\n",
    "    print(f\"{model_name}: R^2 score = {scores['R^2']:.4f}, RMSE = {scores['RMSE']:.4f}\")\n",
    "#    print(f\"{model_name}: R^2 score = {round(scores['R^2'],2)}, RMSE = {scores['RMSE']:.4f}\")\n",
    "\n",
    "# Optionally display trained models\n",
    "#for name, model in trained_models:\n",
    "#    print(f\"Trained model: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "893f2ba8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'setting_std' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#checking this x_data is standardized or not. \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#checking this x_data is standardized or not. \u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43msetting_std\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      4\u001b[0m     df_model_scores \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_dict(model_scores, orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m     df_model_scores_sort \u001b[38;5;241m=\u001b[39m df_model_scores\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRMSE\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'setting_std' is not defined"
     ]
    }
   ],
   "source": [
    "#checking this x_data is standardized or not. \n",
    "#checking this x_data is standardized or not. \n",
    "if setting_std == 0:\n",
    "    df_model_scores = pd.DataFrame.from_dict(model_scores, orient='index')\n",
    "    df_model_scores_sort = df_model_scores.sort_values(by='RMSE', ascending=True)\n",
    "    print('x_data is NOT standardized')\n",
    "elif setting_std == 1:\n",
    "    df_model_scores = pd.DataFrame.from_dict(model_scores, orient='index')\n",
    "    df_model_scores_sort = df_model_scores.sort_values(by='RMSE', ascending=True)\n",
    "    print('x_data is Standardized')\n",
    "else:\n",
    "    print('Something is wrong')\n",
    "\n",
    "df_model_scores_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43c83c5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trained_models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrained_models\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trained_models' is not defined"
     ]
    }
   ],
   "source": [
    "trained_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71be12bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data is Standardized\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Model1</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model2</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        RMSE   R2\n",
       "Model1   0.5  0.9\n",
       "Model2   0.6  0.8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking this x_data is standardized or not. \n",
    "#checking this x_data is standardized or not. \n",
    "if setting_std == 0:\n",
    "    df_model_scores = pd.DataFrame.from_dict(model_scores, orient='index')\n",
    "    df_model_scores_sort = df_model_scores.sort_values(by='RMSE', ascending=True)\n",
    "    print('x_data is NOT standardized')\n",
    "elif setting_std == 1:\n",
    "    df_model_scores = pd.DataFrame.from_dict(model_scores, orient='index')\n",
    "    df_model_scores_sort = df_model_scores.sort_values(by='RMSE', ascending=True)\n",
    "    print('x_data is Standardized')\n",
    "else:\n",
    "    print('Something is wrong')\n",
    "\n",
    "df_model_scores_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b35b0a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OneHotEncoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Apply One-Hot Encoding\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m encoder \u001b[38;5;241m=\u001b[39m \u001b[43mOneHotEncoder\u001b[49m(sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, handle_unknown\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m encoded_categorical_data \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mfit_transform(df[categorical_cols])\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Create feature names for the encoded columns\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'OneHotEncoder' is not defined"
     ]
    }
   ],
   "source": [
    "# Apply One-Hot Encoding\n",
    "encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "encoded_categorical_data = encoder.fit_transform(df[categorical_cols])\n",
    "\n",
    "# Create feature names for the encoded columns\n",
    "encoded_feature_names = [f\"{col}_{val}\" for col, vals in zip(categorical_cols, encoder.categories_) for val in vals]\n",
    "\n",
    "# Create a DataFrame from the encoded data\n",
    "encoded_df = pd.DataFrame(encoded_categorical_data, columns=encoded_feature_names)\n",
    "\n",
    "# Drop original categorical columns and join encoded data\n",
    "df = df.drop(categorical_cols, axis=1)\n",
    "df = df.join(encoded_df)\n",
    "\n",
    "# Now df has all numeric columns\n",
    "x_data = df.drop('Total Volume of Bookings', axis=1)\n",
    "y_data = df['Total Volume of Bookings']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da41b57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikishah/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.158e+03, tolerance: 8.453e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/nikishah/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.601e+03, tolerance: 2.442e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/nikishah/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.687e+02, tolerance: 2.139e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/nikishah/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.588e+03, tolerance: 2.554e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/nikishah/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.637e+03, tolerance: 2.615e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/nikishah/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.127e+03, tolerance: 8.453e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/nikishah/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.490e+03, tolerance: 2.442e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/nikishah/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.444e+02, tolerance: 2.139e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/nikishah/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.483e+03, tolerance: 2.554e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/nikishah/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.466e+03, tolerance: 2.615e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/nikishah/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.204e+02, tolerance: 8.453e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/nikishah/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.911e+02, tolerance: 2.442e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/nikishah/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.684e+02, tolerance: 2.139e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/nikishah/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.785e+02, tolerance: 2.554e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/nikishah/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.742e+02, tolerance: 2.615e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/nikishah/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.129e+03, tolerance: 8.453e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/nikishah/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.491e+03, tolerance: 2.442e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/nikishah/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.446e+02, tolerance: 2.139e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/nikishah/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.487e+03, tolerance: 2.554e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/nikishah/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.631e+03, tolerance: 2.615e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/nikishah/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.139e+03, tolerance: 8.453e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/nikishah/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.547e+03, tolerance: 2.442e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/nikishah/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.485e+02, tolerance: 2.139e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikishah/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.551e+03, tolerance: 2.554e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, make_scorer\n",
    "from sklearn.linear_model import Lasso, OrthogonalMatchingPursuit, ARDRegression\n",
    "from math import sqrt\n",
    "\n",
    "# Custom scorer for RMSE\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "# Making scorer for R2 and RMSE\n",
    "r2_scorer = make_scorer(r2_score)\n",
    "rmse_scorer = make_scorer(rmse, greater_is_better=False)\n",
    "\n",
    "# Define setting_std\n",
    "setting_std = 0  # Set to 0 for x_data not standardized, 1 for standardized\n",
    "\n",
    "# Initialize an empty list to store models and their parameters\n",
    "models_params = []\n",
    "search_results = []\n",
    "\n",
    "if setting_std == 0 or setting_std == 1:\n",
    "    models_params = [\n",
    "        ('Lasso Regression', Lasso(), {\n",
    "            'alpha': np.logspace(-4, 4, num=9),\n",
    "            'max_iter': np.arange(100, 1001, 100)\n",
    "        }),\n",
    "        ('Orthogonal Matching Pursuit', OrthogonalMatchingPursuit(), {\n",
    "            'n_nonzero_coefs': np.arange(1, 10, 1)\n",
    "        }),\n",
    "        ('Automatic Relevance Detection (ARD)', ARDRegression(), {\n",
    "            'n_iter': np.arange(100, 501, 100),\n",
    "            'tol': np.logspace(-4, -1, num=4),\n",
    "            'alpha_1': np.logspace(-6, -1, num=6),\n",
    "            'alpha_2': np.logspace(-6, -1, num=6),\n",
    "            'lambda_1': np.logspace(-6, -1, num=6),\n",
    "            'lambda_2': np.logspace(-6, -1, num=6)\n",
    "        })\n",
    "    ]\n",
    "\n",
    "# Run RandomizedSearchCV for each model\n",
    "for model_name, model, params in models_params:\n",
    "    random_search = RandomizedSearchCV(model, params, n_iter=10, cv=5, scoring={'R2': r2_scorer, 'RMSE': rmse_scorer}, refit='R2', random_state=42)\n",
    "    random_search.fit(x_data, y_data)\n",
    "    search_results.append((model_name, random_search))\n",
    "\n",
    "# Output setting information\n",
    "if setting_std == 0:\n",
    "    print('##### x_data is not standardized #####')\n",
    "elif setting_std == 1:\n",
    "    print('##### x_data is standardized #####')\n",
    "else:\n",
    "    print(\"##### something wrong #####\")\n",
    "    \n",
    "# Display the results for each model\n",
    "for model_name, result in search_results:\n",
    "    print(f\"Results for {model_name}:\")\n",
    "    print(f\"Best R2 score: {result.best_score_}\")\n",
    "    # Retrieve the RMSE from cv_results_\n",
    "    mean_rmse = result.cv_results_['mean_test_RMSE'][result.best_index_]\n",
    "    print(f\"Corresponding RMSE: {-mean_rmse}\")  # RMSE is negated to make it positive\n",
    "    print(f\"Best parameters: {result.best_params_}\\n\")\n",
    "\n",
    "for model_name, model, params in models_params:\n",
    "    try:\n",
    "        random_search = RandomizedSearchCV(model, params, n_iter=10, cv=5, scoring={'R2': r2_scorer, 'RMSE': rmse_scorer}, refit='R2', random_state=42, error_score='raise')\n",
    "        random_search.fit(x_data, y_data)\n",
    "        search_results.append((model_name, random_search))\n",
    "    except Exception as e:\n",
    "        print(f\"Error with {model_name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996bacfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikishah/anaconda3/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "/Users/nikishah/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.158e+03, tolerance: 8.453e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/nikishah/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.601e+03, tolerance: 2.442e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/nikishah/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.687e+02, tolerance: 2.139e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/nikishah/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.588e+03, tolerance: 2.554e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/nikishah/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.637e+03, tolerance: 2.615e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/nikishah/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.127e+03, tolerance: 8.453e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/nikishah/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.485e+03, tolerance: 2.442e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/nikishah/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.439e+02, tolerance: 2.139e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/nikishah/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.482e+03, tolerance: 2.554e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/nikishah/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.437e+03, tolerance: 2.615e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/nikishah/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.203e+02, tolerance: 8.453e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/nikishah/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.911e+02, tolerance: 2.442e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/nikishah/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.683e+02, tolerance: 2.139e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/nikishah/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.784e+02, tolerance: 2.554e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/nikishah/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.742e+02, tolerance: 2.615e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/nikishah/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.129e+03, tolerance: 8.453e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/nikishah/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.485e+03, tolerance: 2.442e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/nikishah/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.440e+02, tolerance: 2.139e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/nikishah/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.487e+03, tolerance: 2.554e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/nikishah/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.627e+03, tolerance: 2.615e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/nikishah/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.139e+03, tolerance: 8.453e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, make_scorer\n",
    "from sklearn.linear_model import Lasso, OrthogonalMatchingPursuit, ARDRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from math import sqrt\n",
    "\n",
    "# Load your data into a DataFrame\n",
    "df = pd.read_csv('/Users/nikishah/Desktop/training_data.csv')  # Replace 'your_data.csv' with your actual file path\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# Apply One-Hot Encoding\n",
    "encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "encoded_categorical_data = encoder.fit_transform(df[categorical_cols])\n",
    "\n",
    "# Create feature names for the encoded columns\n",
    "encoded_feature_names = [f\"{col}_{val}\" for col, vals in zip(categorical_cols, encoder.categories_) for val in vals]\n",
    "\n",
    "# Create a DataFrame from the encoded data\n",
    "encoded_df = pd.DataFrame(encoded_categorical_data, columns=encoded_feature_names)\n",
    "\n",
    "# Drop original categorical columns and join encoded data\n",
    "df = df.drop(categorical_cols, axis=1)\n",
    "df = df.join(encoded_df)\n",
    "\n",
    "# Prepare x_data and y_data\n",
    "x_data = df.drop('Total Volume of Bookings', axis=1)\n",
    "y_data = df['Total Volume of Bookings']\n",
    "\n",
    "# Custom scorer for RMSE\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "# Making scorer for R2 and RMSE\n",
    "r2_scorer = make_scorer(r2_score)\n",
    "rmse_scorer = make_scorer(rmse, greater_is_better=False)\n",
    "\n",
    "# Initialize an empty list to store models and their parameters\n",
    "models_params = []\n",
    "search_results = []\n",
    "\n",
    "# Define your setting_std variable here (0 or 1)\n",
    "setting_std = 0  # or 1, depending on your data\n",
    "\n",
    "# Define models and their hyperparameters\n",
    "if setting_std == 0 or setting_std == 1:\n",
    "    models_params = [\n",
    "        # ... (rest of your models and parameters)\n",
    "    ]\n",
    "\n",
    "\n",
    "# Define models and their hyperparameters\n",
    "if setting_std == 0 or setting_std == 1:\n",
    "    models_params = [\n",
    "        ('Lasso Regression', Lasso(), {\n",
    "            'alpha': np.logspace(-4, 4, num=9),\n",
    "            'max_iter': np.arange(100, 1001, 100)\n",
    "        }),\n",
    "        ('Orthogonal Matching Pursuit', OrthogonalMatchingPursuit(), {\n",
    "            'n_nonzero_coefs': np.arange(1, 10, 1)\n",
    "        }),\n",
    "        ('Automatic Relevance Detection (ARD)', ARDRegression(), {\n",
    "            'n_iter': np.arange(100, 501, 100),\n",
    "            'tol': np.logspace(-4, -1, num=4),\n",
    "            'alpha_1': np.logspace(-6, -1, num=6),\n",
    "            'alpha_2': np.logspace(-6, -1, num=6),\n",
    "            'lambda_1': np.logspace(-6, -1, num=6),\n",
    "            'lambda_2': np.logspace(-6, -1, num=6)\n",
    "        })\n",
    "    ]\n",
    "\n",
    "# Run RandomizedSearchCV for each model with error handling\n",
    "for model_name, model, params in models_params:\n",
    "    try:\n",
    "        random_search = RandomizedSearchCV(model, params, n_iter=10, cv=5, scoring={'R2': r2_scorer, 'RMSE': rmse_scorer}, refit='R2', random_state=42)\n",
    "        random_search.fit(x_data, y_data)\n",
    "        search_results.append((model_name, random_search))\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred with {model_name}: {e}\")\n",
    "\n",
    "# Output setting information\n",
    "if setting_std == 0:\n",
    "    print('##### x_data is not standardized #####')\n",
    "elif setting_std == 1:\n",
    "    print('##### x_data is standardized #####')\n",
    "else:\n",
    "    print(\"##### something wrong #####\")\n",
    "\n",
    "# Check if any results were found\n",
    "if not search_results:\n",
    "    print(\"No results were found. Please check your models and data.\")\n",
    "else:\n",
    "    # Display the results for each model\n",
    "    for model_name, result in search_results:\n",
    "        print(f\"Results for {model_name}:\")\n",
    "        print(f\"Best R2 score: {result.best_score_}\")\n",
    "        mean_rmse = result.cv_results_['mean_test_RMSE'][result.best_index_]\n",
    "        print(f\"Corresponding RMSE: {-mean_rmse}\")  # RMSE is negated to make it positive\n",
    "        print(f\"Best parameters: {result.best_params_}\\n\")\n",
    "\n",
    "        print(\"Categorical Columns:\", categorical_cols)\n",
    "print(\"Models and Parameters:\", models_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24fb8649",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikishah/anaconda3/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Assuming 'df' is your DataFrame\n",
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# Apply One-Hot Encoding\n",
    "encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "encoded_categorical_data = encoder.fit_transform(df[categorical_cols])\n",
    "\n",
    "# Create feature names for the encoded columns\n",
    "encoded_feature_names = encoder.get_feature_names_out(categorical_cols)\n",
    "\n",
    "# Create a DataFrame from the encoded data\n",
    "encoded_df = pd.DataFrame(encoded_categorical_data, columns=encoded_feature_names)\n",
    "\n",
    "# Drop original categorical columns and join encoded data\n",
    "df = df.drop(categorical_cols, axis=1)\n",
    "df = pd.concat([df, encoded_df], axis=1)\n",
    "\n",
    "# Now, split the data\n",
    "X = df.drop('Total Volume of Bookings', axis=1)  # Replace with your target variable\n",
    "y = df['Total Volume of Bookings']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e12d0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 45 candidates, totalling 225 fits\n",
      "Best RMSE: 5.051730970807861\n",
      "Best Parameters: {'model__max_depth': 10, 'model__min_samples_leaf': 1, 'model__min_samples_split': 5}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# Assuming 'df' is your DataFrame and 'Total Volume of Bookings' is your target variable\n",
    "# df = pd.read_csv('your_data.csv')  # Replace 'your_data.csv' with your actual file path\n",
    "\n",
    "# Preprocessing steps (assuming you have categorical and numerical columns)\n",
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "numerical_cols = df.select_dtypes(include=['number']).columns.drop('Total Volume of Bookings')\n",
    "\n",
    "# Preprocessors\n",
    "numerical_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Split the data\n",
    "X = df.drop('Total Volume of Bookings', axis=1)\n",
    "y = df['Total Volume of Bookings']\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Parameters for Decision Tree Regressor\n",
    "params = {\n",
    "    'model__max_depth': [None, 5, 10, 15, 20],\n",
    "    'model__min_samples_split': [2, 5, 10],\n",
    "    'model__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Create the pipeline for the Decision Tree Regressor\n",
    "decision_tree_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                         ('model', DecisionTreeRegressor(random_state=0))])\n",
    "\n",
    "# Setup the grid search\n",
    "grid_search = GridSearchCV(decision_tree_pipeline, param_grid=params, cv=5, scoring='neg_mean_squared_error', verbose=1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best estimator\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the validation set\n",
    "best_preds = best_model.predict(X_valid)\n",
    "best_rmse = mean_squared_error(y_valid, best_preds, squared=False)  # Get RMSE\n",
    "\n",
    "# Output the performance of the best model and the best parameters\n",
    "print(\"Best RMSE:\", best_rmse)\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06cbf83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 45 candidates, totalling 225 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5.051730970807861,\n",
       " {'model__max_depth': 10,\n",
       "  'model__min_samples_leaf': 1,\n",
       "  'model__min_samples_split': 5})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Parameters for Decision Tree Regressor\n",
    "params = {\n",
    "    'model__max_depth': [None, 5, 10, 15, 20],\n",
    "    'model__min_samples_split': [2, 5, 10],\n",
    "    'model__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Create the pipeline for the Decision Tree Regressor\n",
    "decision_tree_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                         ('model', DecisionTreeRegressor(random_state=0))])\n",
    "\n",
    "# Setup the grid search\n",
    "grid_search = GridSearchCV(decision_tree_pipeline, param_grid=params, cv=5, scoring='neg_mean_squared_error', verbose=1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best estimator\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the validation set\n",
    "best_preds = best_model.predict(X_valid)\n",
    "best_rmse = mean_squared_error(y_valid, best_preds, squared=False)  # Get RMSE\n",
    "\n",
    "# Output the performance of the best model and the best parameters\n",
    "best_rmse, grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "02b2d4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5.156120923758276,\n",
       " {'model__max_depth': 20,\n",
       "  'model__min_samples_leaf': 1,\n",
       "  'model__min_samples_split': 5,\n",
       "  'model__n_estimators': 100})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Define the RandomForestRegressor pipeline with preprocessor\n",
    "rf_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('model', RandomForestRegressor(random_state=0))])\n",
    "\n",
    "# Hyperparameters for Random Forest Regressor\n",
    "rf_params = {\n",
    "    'model__n_estimators': [50, 100, 150],\n",
    "    'model__max_depth': [10, 20, 30],\n",
    "    'model__min_samples_split': [2, 5],\n",
    "    'model__min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# Setup the grid search for Random Forest Regressor\n",
    "rf_grid_search = GridSearchCV(rf_pipeline, param_grid=rf_params, cv=5, scoring='neg_mean_squared_error', verbose=1, n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "rf_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best estimator\n",
    "rf_best_model = rf_grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best Random Forest model on the validation set\n",
    "rf_best_preds = rf_best_model.predict(X_valid)\n",
    "rf_best_rmse = mean_squared_error(y_valid, rf_best_preds, squared=False)  # Get RMSE\n",
    "\n",
    "# Output the performance of the best Random Forest model and the best parameters\n",
    "rf_best_rmse, rf_grid_search.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a9d2557",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This LinearRegression instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 13\u001b[0m\n\u001b[1;32m      7\u001b[0m X_test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/nikishah/Desktop/testing_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Replace 'test_data.csv' with the actual filename\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Apply preprocessing steps to the test data\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Make sure to use the same preprocessing steps applied to your training data\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Make predictions on the preprocessed test data\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mfinal_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Create a DataFrame with entry IDs (if available) and predictions\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Replace 'entry_ids_test' with the actual entry IDs column name\u001b[39;00m\n\u001b[1;32m     17\u001b[0m predictions_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentry_id\u001b[39m\u001b[38;5;124m'\u001b[39m: entry_ids_test, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal Volume of Bookings\u001b[39m\u001b[38;5;124m'\u001b[39m: predictions})\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_base.py:386\u001b[0m, in \u001b[0;36mLinearModel.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    373\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;124;03m    Predict using the linear model.\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;124;03m        Returns predicted values.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decision_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_base.py:367\u001b[0m, in \u001b[0;36mLinearModel._decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decision_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m--> 367\u001b[0m     \u001b[43mcheck_is_fitted\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(X, accept_sparse\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoo\u001b[39m\u001b[38;5;124m\"\u001b[39m], reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m safe_sparse_dot(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_\u001b[38;5;241m.\u001b[39mT, dense_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/utils/validation.py:1461\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m   1458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not an estimator instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (estimator))\n\u001b[1;32m   1460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_fitted(estimator, attributes, all_or_any):\n\u001b[0;32m-> 1461\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(msg \u001b[38;5;241m%\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(estimator)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m})\n",
      "\u001b[0;31mNotFittedError\u001b[0m: This LinearRegression instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming X_test is your test data (features) and entry_ids_test is the corresponding entry IDs (if available)\n",
    "# Assuming final_model is your trained final model\n",
    "\n",
    "# Load your test data\n",
    "X_test = pd.read_csv('/Users/nikishah/Desktop/testing_data.csv')  # Replace 'test_data.csv' with the actual filename\n",
    "\n",
    "# Apply preprocessing steps to the test data\n",
    "# Make sure to use the same preprocessing steps applied to your training data\n",
    "\n",
    "# Make predictions on the preprocessed test data\n",
    "predictions = final_model.predict(X_test)\n",
    "\n",
    "# Create a DataFrame with entry IDs (if available) and predictions\n",
    "# Replace 'entry_ids_test' with the actual entry IDs column name\n",
    "predictions_df = pd.DataFrame({'entry_id': entry_ids_test, 'Total Volume of Bookings': predictions})\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "predictions_df.to_csv('submission new.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb91426",
   "metadata": {},
   "source": [
    "Development of Candidate Models and Selection of Final Models\n",
    "Several regression models were assessed for their accuracy in predicting the \"Total Volume of Bookings\" as part of the selection process. The R2 score and Root Mean Squared Error (RMSE) of each model were evaluated, with special attention paid to the model's interpretability and ability to handle high-dimensional feature spaces. Because of its insightful feature selection, interpretability, and strong performance, the LARS model turned out to be the best option.\n",
    "\n",
    "Our decision was further supported by the cross-validation results, which showed that the LARS model is reliable and can be applied to previously untested data because it continuously achieves competitive R scores across folds. The model's capacity to pinpoint important elements that influence booking volumes and offer useful information for improving advertising stratergies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3c7c01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
